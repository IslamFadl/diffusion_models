{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ceQ4WryRIVvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9ec5c8-32cd-42e5-e927-a43fb2ad8de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.32.2)\n",
            "Collecting diffusers\n",
            "  Downloading diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, diffusers, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.32.2\n",
            "    Uninstalling diffusers-0.32.2:\n",
            "      Successfully uninstalled diffusers-0.32.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 diffusers-0.33.1 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets diffusers einops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Diffusion\n",
        "\n",
        "This is a notebook to inspect some interesting aspects of Diffusion Model training from a simplified point of view. There are multiple superb blogs and papers discussing the underlying mathematics and derivations of diffusion models. However, sparingly few explain in layman terms how the hyper parameters they chose relate to this theory.\n",
        "\n",
        "So this notebook will take the opposite approach. Under the assumption that the math is indeed correct and is correctly implemented by an underlying library (which we will attempt NOT to modify unless needed), we will take an empirical approach and try various things to try to make-or-break a diffusion training recipe.\n",
        "\n",
        "Hopefully, this will help us determine sosme simplified ways to deal with trivial diffusion training tasks on toy data. From there, one can follow the literature and learnings from this process, and scale up to real world datasets.\n",
        "\n",
        "In this notebook, we will develop a standard model, use a simple pipeline from Hugging Face Diffusers, and train a model on a single multivariate sequence, try to train it to convergence (MSE ~ 1e-3 or lower on CPU training), generate some samples from the trained model, and in due process, hopefully understand all minute details required to enable stable training (at least on a toy dataset).\n",
        "\n",
        "**Note: This is NOT a notebook to understand the underlying mathematics of Diffusion Models.** As mentioned above, there are plenty of much more appropriate resources in various posts and papers.\n",
        "\n",
        "Topics covered :\n",
        "\n",
        "- Importance of model design\n",
        "- Importance of normalization of data\n",
        "- Role played by $\\beta$ when setting up Diffusion Scheduler\n",
        "- Creation of custom Sampling Pipeline for Sequence generation task\n",
        "- How to overfit a single training sample with Diffusion Training\n",
        "- Improving the efficiency of single sample fitting\n",
        "- Sampling\n"
      ],
      "metadata": {
        "id": "MW2Cg30DrWIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from einops import rearrange, reduce\n",
        "from einops.layers.torch import Rearrange\n",
        "from dataclasses import dataclass\n",
        "from typing import Union, Optional\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "5E0I7KQyrZZB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture - GPT\n",
        "\n",
        "Commonly, U-Nets are used in literature with various modifications made to either location of attention blocks, number of strides, number of filters in each layer, how to add conditional information etc.\n",
        "\n",
        "Vision Transformers show that training on image patches and applying standard Transformer layers works as well. [Diffusion Transformer](https://arxiv.org/abs/2212.09748) takes this idea and applies Transformers to Diffusion Generative modelling task. It requires some minor modifications and is applied to latent space (effectively doing latent diffusion to avoid the cost of attention blocks on high resolution images; though the authors suggest it can be modified to apply directly on image diffusion rather than latents).\n",
        "\n",
        "For a single sample, we do not need such complexity. The single datapoint will be a 12-channel 26-timestep sequence, and the memory cost is trivially small enough to not even require subsampling. So we will directly apply a very small GPT network to the problem. Note: The code for the model is ported from nanoGPT.\n",
        "\n",
        "This still apparantly posed sufficient problems which I'll discuss below:\n",
        "\n",
        "- ``SelfAttention`` : Causal padding will ensure model fails to train on even a single sample. Had to remove causal padding entirely.\n",
        "\n",
        "- ``Block`` : Unlike Conv-based U-Nets where the Diffusion Time features are passed to every block, GPT does not need this. In fact, while it can be added back by uncommenting the line in forward - it will actually cause the model to converge slower.\n",
        "\n",
        "- ``SinusoidalPosEmb`` : While the general positional embedding can be either Absolute Sinusodial embeddings, or learned embeddings, when discussing the embedding for the Diffusion timestep itself - Sinusodial PE was quite necessary. Learned embedding did far worse (at least for single sample diffusion). Interestingly, swapping sine() with cos() in the final concat step has seemingly no effect on training for a single sample.\n",
        "\n",
        "- ``PositionalEmbedding`` : Some works apply a small FF network after the Sinusodial Positional Embedding. This was not strictly necessary for a single image, but does help if you try to fit an entire dataset with a tiny number of parameters.\n",
        "\n",
        "- ``GPT - Diffusion Time Embedding`` : For the GPT code, you must provide some form of diffusion time embedding to denote which timestep the current sample is at. For example, if you follow the line ``time_emb = self.transformer.wpe(time).unsqueeze(1) # time embeddings of shape (time, 1, D); len(time) = B`` with ``time_emb = torch.zeros_like(time_emb)`` - you are guarenteed to be unable to train your model. Nothing I tried could make the model train.\n",
        "\n",
        "- ``GPT - Positional Embedding (Sequence Embedding)`` : Even for this simple task of overfitting a single sample, positional encodings (learned or sinusodial) are mandatory. Try to zero them out (or simply remove seq_emb from the addition in ``x = self.transformer.drop(inp_emb + seq_emb)`` to completely halt training. Nothing I tried could make the loss reduce below a very high value.\n",
        "\n",
        "- ``Param Count``: For a single sample, we can get away with very few trainable parameters (discounting the parameters of the large embedding matrices since we do not train on all timesteps).\n",
        "\n",
        "Reference: https://github.com/karpathy/nanoGPT"
      ],
      "metadata": {
        "id": "tzAYNcLLrkkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def new_gelu(x):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd: int, n_head: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(n_embd, 4 * n_embd)\n",
        "        self.c_proj  = nn.Linear(4 * n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = new_gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd: int, n_head: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = SelfAttention(n_embd, n_head, dropout)\n",
        "        self.ln_2 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = MLP(n_embd, dropout)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # Try uncommenting below block to see if adding the diffusion timestep\n",
        "        # embedding to every block benefits the quality of samples or not.\n",
        "        # x = x + t\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Added block\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim, max_timesteps: int = 10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_timesteps = float(max_timesteps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(self.max_timesteps) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, max_timesteps: int):\n",
        "        super().__init__()\n",
        "        self.pe = SinusoidalPosEmb(dim=dim, max_timesteps=max_timesteps)\n",
        "\n",
        "        # Try uncommenting the follwing and the forward step to see whether learned\n",
        "        # embeddings lead to a better result.\n",
        "\n",
        "        # self.ff1 = nn.Linear(dim, dim * 4)\n",
        "        # self.gelu = nn.GELU()\n",
        "        # self.ff2 = nn.Linear(dim * 4, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pe(x)\n",
        "        # out = self.ff2(self.gelu(self.ff1(out)))\n",
        "        return out\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    dim: int\n",
        "    block_size: int = 10000\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 128\n",
        "    dropout: float = 0.1\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            input_proj = nn.Linear(config.dim, config.n_embd, bias=False),\n",
        "            wte = nn.Embedding(config.block_size, config.n_embd),\n",
        "            wpe = PositionalEmbedding(config.n_embd, max_timesteps=config.block_size),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(n_embd=config.n_embd, n_head=config.n_head, dropout=config.dropout) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "            output_proj = nn.Linear(config.n_embd, config.dim, bias=False),\n",
        "        ))\n",
        "\n",
        "        # report number of parameters\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        embd_params = sum(p.numel() for p in self.transformer.wte.parameters())\n",
        "        print(\"Total number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "        print(\"Non-Embedding number of parameters: %.2fM\" % ((n_params - embd_params)/1e6,))\n",
        "        print(\"Embedding number of parameters: %.2fM\" % (embd_params/1e6,))\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        device = x.device\n",
        "        b, d, t = x.size()  # [B, C, T]\n",
        "\n",
        "        assert d == self.config.dim, f\"Expected input dim {self.config.dim} got {d}\"\n",
        "\n",
        "        time = time.to(dtype=torch.long, device=device) # torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "        if time.ndim == 0:\n",
        "          time = time.unsqueeze(0).repeat(b)\n",
        "\n",
        "        assert x.size(0) == time.size(0), f\"Time ({len(time)}) and input batch size ({len(x)}) must match\"\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        x_input = x\n",
        "\n",
        "        x = x.transpose(1, 2)  # [B, T, C]\n",
        "        inp_emb = self.transformer.input_proj(x)  # [B, T, D]\n",
        "        seq_emb = self.transformer.wte(torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)) # [1, t, D]\n",
        "        time_emb = self.transformer.wpe(time).unsqueeze(1) # time embeddings of shape (time, 1, D); len(time) = B\n",
        "\n",
        "        # You can try to remove `seq_emb` from the following line to see the model fail to converge and generate any samples at all.\n",
        "        x = self.transformer.drop(inp_emb + seq_emb)\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            # To see the necessity of the diffusion time emb, you can multiply it\n",
        "            # with 0 to get a model that has no conditioning on the diffusion timestep.\n",
        "            x = block(x, time_emb)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        x_out = self.transformer.output_proj(x)\n",
        "        x_out = x_out.transpose(1, 2)\n",
        "\n",
        "        x = x_input + x_out\n",
        "\n",
        "        return x\n",
        "\n",
        "    # Add two properties used by HF Sampling Pipeline\n",
        "    @property\n",
        "    def dtype(self):\n",
        "      return next(self.parameters()).dtype\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "      return next(self.parameters()).device"
      ],
      "metadata": {
        "id": "2tUeGSx-riiN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing a Dataset\n",
        "\n",
        "In this example, we will be using a tiny speech recognition dataset for Japanese vowels rather than the usual image diffusion. We chose such a task specifically because image diffusion has been studied extensively, and usually training even a single image diffusion model on CPU would take a non trivial amount of compute or memory.\n",
        "\n",
        "We will use the [Japanese Vowels dataset](https://archive.ics.uci.edu/ml/datasets/Japanese+Vowels), where each sample is a multivariate time series comprising of 12 LPC cepstrum coefficients over a of 26 frames. The amount of compute required to train on this data is trivial compared to even a single 256x256 image.\n",
        "\n",
        "Most importantly, though the dataset has 12 coefficients, we can easily visualize a single channel as a basic line plot, and we will later see that this sequence has a distinctive signature which we will task the diffusion model of learning."
      ],
      "metadata": {
        "id": "7uNS-6OKr7YT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downlaod processed dataset\n",
        "\n",
        "This dataset is already processed as part of prior research for multivariate time series classification tasks at my repository, so we will simply download and use the processed numpy arrays."
      ],
      "metadata": {
        "id": "4v_uSE_eDy3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"/content/JapaneseVowels-20180329T000739Z-001.zip\"):\n",
        "  !wget https://github.com/titu1994/MLSTM-FCN/releases/download/v1.0/JapaneseVowels-20180329T000739Z-001.zip"
      ],
      "metadata": {
        "id": "RiHwm8qSrs-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54337043-05f9-47df-f83b-21ebde211acb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-20 20:27:57--  https://github.com/titu1994/MLSTM-FCN/releases/download/v1.0/JapaneseVowels-20180329T000739Z-001.zip\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/117737292/ff55440e-32be-11e8-96f9-99b45c89fb54?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250420%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250420T202758Z&X-Amz-Expires=300&X-Amz-Signature=2a06ba37ff45ef7113b187d4779b18bfa33f742db8b99a54f539c4d57d350b92&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DJapaneseVowels-20180329T000739Z-001.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-04-20 20:27:58--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/117737292/ff55440e-32be-11e8-96f9-99b45c89fb54?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250420%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250420T202758Z&X-Amz-Expires=300&X-Amz-Signature=2a06ba37ff45ef7113b187d4779b18bfa33f742db8b99a54f539c4d57d350b92&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3DJapaneseVowels-20180329T000739Z-001.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1865025 (1.8M) [application/octet-stream]\n",
            "Saving to: ‘JapaneseVowels-20180329T000739Z-001.zip’\n",
            "\n",
            "JapaneseVowels-2018 100%[===================>]   1.78M  7.09MB/s    in 0.3s    \n",
            "\n",
            "2025-04-20 20:28:00 (7.09 MB/s) - ‘JapaneseVowels-20180329T000739Z-001.zip’ saved [1865025/1865025]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "dataset_root = \"dataset/\"\n",
        "os.makedirs(dataset_root, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile('JapaneseVowels-20180329T000739Z-001.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall(dataset_root)"
      ],
      "metadata": {
        "id": "uZCa4ZPyr_xJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize the dataset\n",
        "\n",
        "In almost all literature for diffusion models, the standard practice is to normalize the input to be within the range [-1, 1], so we will do the same for this dataset.\n",
        "\n",
        "A simple study can be to change this to mean-std normalization instead."
      ],
      "metadata": {
        "id": "QOX6kJ5lEEpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "jp_root = os.path.join(dataset_root, \"JapaneseVowels\")\n",
        "\n",
        "x_train = np.load(os.path.join(jp_root, \"X_train.npy\"))  # [B, C, T]\n",
        "x_test = np.load(os.path.join(jp_root, \"X_test.npy\"))  # [B, C, T]\n",
        "\n",
        "# Normalize dataset to [-1, 1] range\n",
        "x_train = (x_train - x_train.min(-1, keepdims=True)) / (x_train.max(-1, keepdims=True) - x_train.min(-1, keepdims=True))\n",
        "x_train = 2 * x_train - 1\n",
        "\n",
        "x_test = (x_test - x_test.min(-1, keepdims=True)) / (x_test.max(-1, keepdims=True) - x_test.min(-1, keepdims=True))\n",
        "x_test = 2 * x_test - 1\n",
        "\n",
        "# Convert dataset to tensors\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "\n",
        "channels = x_train.shape[1]\n",
        "timesteps = x_train.shape[2]\n",
        "\n",
        "print(\"Loaded X Train :\", x_train.shape, \"Max\", x_train.max(), \"Min\", x_train.min())\n",
        "print(\"Loaded X Test  :\", x_test.shape, \"Max\", x_test.max(), \"Min\", x_test.min())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FPiSX5usBm5",
        "outputId": "6cf6a8a2-748b-4dba-90b4-431d919bd4d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded X Train : torch.Size([270, 12, 26]) Max tensor(1.) Min tensor(-1.)\n",
            "Loaded X Test  : torch.Size([370, 12, 26]) Max tensor(1.) Min tensor(-1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check model forward pass\n",
        "\n",
        "Given a dataset and a model, lets make sure that the forward pass works as expected.\n",
        "\n",
        "Note: For a single sample, we wont be using millions of parameters, a few hundred thousand (or less) will be sufficient."
      ],
      "metadata": {
        "id": "qa2r30U8sK4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig(dim=channels, n_embd=32, n_head=4, n_layer=4, dropout=0.0)\n",
        "model = GPT(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMaQiIm8sEQL",
        "outputId": "ae0fb564-a3ae-46a9-9e3a-aa3334ce1a0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 0.37M\n",
            "Non-Embedding number of parameters: 0.05M\n",
            "Embedding number of parameters: 0.32M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  tmp_ip = x_train[0:8]\n",
        "  out = model(tmp_ip, time=torch.tensor(list(range(8)), dtype=torch.long))\n",
        "  print(\"Input  :\", tmp_ip.shape)\n",
        "  print(\"Output :\", out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEtyWFi6sSZ8",
        "outputId": "27ea1a71-940d-41c6-9a91-456975e02afa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input  : torch.Size([8, 12, 26])\n",
            "Output : torch.Size([8, 12, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Scheduler\n",
        "\n",
        "We will use the simple diffusion scheduler from the [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) that has been implemented in [Hugging Face](https://huggingface.co/docs/diffusers/api/schedulers/ddpm).\n",
        "\n",
        "We will make a small modification to the paper, which is to change the number of diffusion steps from 1000 to something manageable, such as 50 or 100. This is for two purposes -\n",
        "\n",
        "1) There's not much examples out there exploring how to make the train schedule itself require smaller number of diffusion steps - its usually in the order of hundreds or sticks to the defaults in the paper (1000 or more steps).\n",
        "\n",
        "2) Very few works mention exactly how they select the parameters of $\\beta_{start}$ and $\\beta_{end}$ for the linear scheduler, which is commonly used (although there is a shift towards continous time discretization which resolves the matter).\n",
        "\n",
        "-----\n",
        "\n",
        "For this example, since we want to train the model quickly on a single sample only, it would be preferable to limit the amount of compute and optimizer updates required to achieve this. One simple method is to reduce the size of the Markov chain that defines the diffusion process - simply reduce the diffusion timesteps.\n",
        "\n",
        "It is to be noted that the model will learn a better result with more diffusion steps, usually. But it will also exacerbate the inference cost which will require thousands of passes of the model to generate a single batch of images.\n"
      ],
      "metadata": {
        "id": "QXyq51CIsXXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to select $\\beta$ range?\n",
        "\n",
        "First we need to note that $\\beta$ is not directly used for the forward process (when converting data to gaussian noise). Instead we use $\\alpha$ which can be defined as\n",
        "\n",
        "\\begin{align}\n",
        "  \\alpha_t &= 1 - \\beta_t \\\\\n",
        "  \\bar{\\alpha}_t &= \\prod_{s=1}^t \\alpha_s\n",
        "\\end{align}\n",
        "\n",
        "Another thing to note is that optimally, at step $t=T$ where $T$ is the number of diffusion steps, the input data should be nearly gaussian noise $\\mathcal{N}(0, 1)$.\n",
        "\n",
        "However, when we change $T$, we also substantially affect $\\alpha_t$ and $\\bar{\\alpha}_t$, which can lead to the output of the diffusion process not being similar to gaussian noise, thereby making the reverse diffusion process more difficult (intuitively, if your image is not completely noise, you would need a partially noisy image as your initial guess when sampling)."
      ],
      "metadata": {
        "id": "-E4zRgINKYbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DDPMScheduler\n",
        "from diffusers.schedulers.scheduling_ddpm import DDPMSchedulerOutput\n",
        "from diffusers.utils import randn_tensor\n",
        "\n",
        "# Setup Constants\n",
        "DIFFUSION_TIMESTEPS = 50  # Number of Diffusion Steps during training. Note, this is much smaller than ordinary (usually ~1000)\n",
        "beta_start = 0.0001  # Default from paper\n",
        "beta_end = 0.2  # NOTE: This is different to accomodate the much shorter DIFFUSION_TIMESTEPS (Usually ~1000). For 1000 diffusion timesteps, use 0.02.\n",
        "clip_sample = True  # Used for better sample generation\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=DIFFUSION_TIMESTEPS, beta_end=beta_end, clip_sample=clip_sample)\n",
        "noise_scheduler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Vu4Upvh-sVQa",
        "outputId": "1d4242e1-b4e2-473f-d033-2b8938a8ba01"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'randn_tensor' from 'diffusers.utils' (/usr/local/lib/python3.11/dist-packages/diffusers/utils/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c8d7ed532128>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDDPMScheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedulers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduling_ddpm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDDPMSchedulerOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandn_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Setup Constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'randn_tensor' from 'diffusers.utils' (/usr/local/lib/python3.11/dist-packages/diffusers/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the validity of the noise schedule\n",
        "\n",
        "Since we change ``DIFFUSION_TIMESTEPS``, we must in turn modify $\\beta_{end}$ to ensure the diffusion process results in gaussian noise as the output. Remember that from Equation 14 in the DDPM paper, we effectively optimize the following -\n",
        "\n",
        "\\begin{align}\n",
        "  \\| \\left(  ϵ - ϵ_θ (\\sqrt{\\bar{\\alpha_t}} x_0 + \\sqrt{1 - \\bar{\\alpha_t}} ϵ, t ) \\right) \\|^2\n",
        "\\end{align}\n",
        "\n",
        "A simple way to accomplish this is to first modify $T$ to a required value, then to modify $\\beta_{end}$ in order to observe the final values of $\\sqrt{\\bar{\\alpha_t}}$ and $\\sqrt{1 - \\bar{\\alpha_t}}$ to ensure they are close to 0 and 1 respectively. In doing so, the above $ϵ_Θ(.)$ represents gaussian noise alone.\n",
        "\n",
        "You may modify the ``DIFFUSION_TIMESTEPS`` ($T$) and $\\beta_{end}$ parameters above to observe the printed values. If they are not close to 0 and 1 respectively, the model may not converge properly."
      ],
      "metadata": {
        "id": "L8BPbpXds87S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alpha Cumulative Product Ranges"
      ],
      "metadata": {
        "id": "znBf7JzMtJAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.sqrt(noise_scheduler.alphas_cumprod))\n",
        "print(torch.sqrt(1. - noise_scheduler.alphas_cumprod))"
      ],
      "metadata": {
        "id": "0R4Yn1N4s5wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampled Variance Range\n",
        "\n",
        "An important side-effect of increasing $\\beta_{end}$ is the variance added to generated samples during the reverse process.\n",
        "\n",
        "Recall that for the reverse process, we need to compute the predicted variance $\\beta_t$ (See Equation 6 and 7 in the DDPM paper) and sample from it to get the previous sample in the reverse process. We then scale this sample by  $\\sqrt{\\tilde{\\beta}_t}$, such that $\\tilde{\\beta}_t$ is defined as follows -\n",
        "\n",
        "\\begin{equation}\n",
        "   \\tilde{\\beta}_t = \\frac{1 - \\bar{α_{t-1}}}{1 - \\bar{α_t}} \\beta_t\n",
        "\\end{equation}\n",
        "\n",
        "The primary issue is that by scaling up the value of $\\beta_{end}$, we have also scaled up the value of $\\tilde{β}_t$, which affects the reverse diffusion process.\n",
        "\n",
        "For a trivial task as learning a single sample, it may be fine to use a large enough $\\beta_t$, but for any real training of diffusion models, care must be taken to have an appropriate number of diffusion steps to manage the scale of $\\tilde{\\beta}_t$ during the backward process.\n",
        "\n",
        "Note that the final value of $\\tilde{\\beta}_T$ will be equivalent to $\\beta_{end}$, and so we can calculate whether the scaling of gaussian noise will be excessive or not.\n",
        "\n",
        "-----\n",
        "\n",
        "**Note**: Under the DDPM paper's guidance, $T=1000$, and so $β_{end} = 0.02$ so $\\tilde{\\beta}_T = \\sqrt{0.02} = 0.1414 ⋯$."
      ],
      "metadata": {
        "id": "Xv6PqhLftG6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variance_ = []\n",
        "for t in range(noise_scheduler.num_train_timesteps):\n",
        "    alpha_prod_t = noise_scheduler.alphas_cumprod[t]\n",
        "    alpha_prod_t_prev = noise_scheduler.alphas_cumprod[t - 1] if t > 0 else noise_scheduler.one\n",
        "\n",
        "    # For t > 0, compute predicted variance βt (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)\n",
        "    # and sample from it to get previous sample\n",
        "    # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample\n",
        "    variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * noise_scheduler.betas[t]\n",
        "    variance_.append(variance)\n",
        "\n",
        "variance_ = torch.tensor(variance_)\n",
        "print(\"Pred sample Variance:\", variance_.min(), \" -> \", variance_.max())\n",
        "print(\"Pred sample stdev   :\", variance_.sqrt())"
      ],
      "metadata": {
        "id": "46NktscKtD3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visual check of Forward Process of Diffusion\n",
        "\n",
        "Next, lets make sure visually that the forward diffusion step is correct. Below, you can modify $t$ to any integer value between $0$ to $T-1$ and see the progressive addition of gaussian noise to the original sequence (here we select just one channel of the model to make visualization simpler)."
      ],
      "metadata": {
        "id": "ZZVdBHY9tcoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take time step as any value 0 < t < DIFFUSION_TIMESTEPS\n",
        "# you can change this value to see the change of original sample to noise\n",
        "t = torch.tensor([DIFFUSION_TIMESTEPS - 1])\n",
        "sequence = x_train[0:1].clone()  # We will use just a single sample throughout this tutorial\n",
        "\n",
        "g = torch.Generator(device='cpu').manual_seed(0)\n",
        "noise = torch.randn(sequence.shape, generator=g)\n",
        "x_noisy = noise_scheduler.add_noise(sequence, noise, t)\n",
        "\n",
        "plt.plot(sequence[0, 0], label='Sequence Channel 0')\n",
        "plt.plot(x_noisy[0, 0], label='Noisy Channel 0')\n",
        "plt.legend()\n",
        "\n",
        "# If at final step of forward process\n",
        "# X_noisy should have approximately mean=0.0 and std=1.0\n",
        "if t[0] == DIFFUSION_TIMESTEPS - 1:\n",
        "  print(\"X noisy statistics :\", \"mean=\", x_noisy.mean(), \"std=\", x_noisy.std())"
      ],
      "metadata": {
        "id": "yIoKGPEKtYY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "We can create an animation to see the transformation of the input to gaussian noise progressing.\n",
        "\n",
        "Note that with large number of diffusion timesteps, this might take some time to generate."
      ],
      "metadata": {
        "id": "6rGXC-fwK7YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: The following cell can take a long time for large DIFFUSION_STEPS\n",
        "\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from matplotlib import rc\n",
        "from IPython.display import HTML\n",
        "\n",
        "# create the figure and axes objects\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "def animate_forward_process(i):\n",
        "  t = torch.tensor([i])\n",
        "\n",
        "  g = torch.Generator(device='cpu').manual_seed(0)\n",
        "  noise = torch.randn(sequence.shape, generator=g)\n",
        "  x_noisy = noise_scheduler.add_noise(sequence, noise, t)\n",
        "\n",
        "  ax.clear()\n",
        "  ax.set_title(f\"T={i}\")\n",
        "  ax.plot(sequence[0, 0], label='Sequence Channel 0')\n",
        "  ax.plot(x_noisy[0, 0], label='Noisy Channel 0')\n",
        "  ax.legend(loc=\"upper right\")\n",
        "\n",
        "# run the animation\n",
        "anim = FuncAnimation(fig, animate_forward_process, frames=list(range(DIFFUSION_TIMESTEPS)), interval=200, repeat=False)\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "HTML(anim.to_jshtml())"
      ],
      "metadata": {
        "id": "dekgWNi4truB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Diffusion Pipeline for Time Series Generation\n",
        "\n",
        "We will be using the [diffusers](https://huggingface.co/docs/diffusers/index) throughout the notebook in order to abstract away the diffusion process logic. The code for ``DDPM`` can be read in order to understand the paper better.\n",
        "\n",
        "Here, we will write a custom ``DiffusionPipelin`` for time series data, which is mostly a replication of the ``DDPMPipeline`` with some additional arguments."
      ],
      "metadata": {
        "id": "Z62D8aaf0jyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "@dataclass\n",
        "class SequenceDataOutput:\n",
        "  sequences: List[torch.Tensor]\n",
        "  history: List[torch.Tensor]\n",
        "\n",
        "\n",
        "class SequenceDDPMPipeline(DiffusionPipeline):\n",
        "\n",
        "  def __init__(self, model: GPT, scheduler: DDPMScheduler):\n",
        "    super().__init__()\n",
        "    self.register_modules(model=model, scheduler=scheduler)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __call__(self,\n",
        "        batch_size: int = 1,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        num_inference_steps: int = DIFFUSION_TIMESTEPS,\n",
        "        return_dict: bool = True,\n",
        "        return_all: bool = False,  # new flag to return all the diffusion reverse process steps as a list\n",
        "        explicit_noise: torch.Tensor = None,  # We can pass in an explicit loss value to ensure diffusion backward process is working as expected\n",
        "        **kwargs,\n",
        "  ) -> SequenceDataOutput:\n",
        "    self.scheduler: DDPMScheduler\n",
        "    self.model: GPT\n",
        "\n",
        "    # Get the sequence length from the notebook's globals as they were defined above when loading the dataset\n",
        "    # Or use the user defined value if passed.\n",
        "    # NOTE: Since we train on a single sample with exact seq_len and seq_dim, changing this will NOT work during inference.\n",
        "    seq_len = kwargs.get('seq_len', timesteps)\n",
        "    seq_dim = kwargs.get('channels', channels)\n",
        "\n",
        "    # Sample noise as an init to the reverse process, if no noise was explicitly provided\n",
        "    if explicit_noise is None:\n",
        "      sequence_data = torch.randn(batch_size, seq_dim, seq_len, device=self.device, generator=generator)\n",
        "    else:\n",
        "      sequence_data = explicit_noise.to(device=device)\n",
        "\n",
        "    # Set inference timesteps\n",
        "    self.scheduler.set_timesteps(num_inference_steps=num_inference_steps)\n",
        "\n",
        "    # Check whether to mantain the history of all reverse diffusion process steps\n",
        "    if return_all:\n",
        "      history = [sequence_data.cpu()]\n",
        "    else:\n",
        "      history = None\n",
        "\n",
        "    for t in self.progress_bar(self.scheduler.timesteps):\n",
        "        # 1. predict noise model_output\n",
        "        model_output = self.model(sequence_data, t)\n",
        "\n",
        "        # 2. compute previous image: x_t -> x_t-1  # prev_sample\n",
        "        sequence_data = self.scheduler.step(model_output, t, sequence_data, generator=generator).prev_sample\n",
        "\n",
        "        # Optional: Preserve history of intermediate step generation\n",
        "        if history is not None:\n",
        "          history.append(sequence_data.cpu().clone())\n",
        "\n",
        "    # Clamp to allowed prediction range\n",
        "    sequence_data = sequence_data.clamp(min=-1.0, max=1.0)\n",
        "\n",
        "    if not return_dict:\n",
        "        return (sequence_data,)\n",
        "\n",
        "    return SequenceDataOutput(sequences=sequence_data, history=history)"
      ],
      "metadata": {
        "id": "yKA4hqAr0o87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader with a Single Sample\n",
        "\n",
        "To diffuse a single sample, we dont really need to create a PyTorch Dataset or DataLoader, but for ease of use where we may want to experiment with more than one sample (or even the entire dataset), we will write a simple data loader."
      ],
      "metadata": {
        "id": "dtDO5cJLyx-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCHSIZE = 128  # used only if you select more than one sample below"
      ],
      "metadata": {
        "id": "I8CtdtZvvzUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "\n",
        "  def __init__(self, X_train):\n",
        "    super().__init__()\n",
        "\n",
        "    self.data = X_train.clone()\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sample = self.data[idx]  # [C, T]\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "0r4y0-q3y6iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = 0\n",
        "\n",
        "assert 0 <= sample_idx < len(x_train), \"Sample index cannot be > number of samples in train set or < 0!\"\n",
        "dataset = TimeSeriesDataset(x_train[sample_idx: sample_idx + 1])\n",
        "dataloader = DataLoader(dataset, batch_size=BATCHSIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "print(\"Dataset size :\", len(dataset), \"\\nSingle Sample shape: \", dataset[0].shape)"
      ],
      "metadata": {
        "id": "vUqIuiAay98u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Model for training\n",
        "\n",
        "The model will learn on a single sample, so we do not need to have too many parameters or too much hyper parameter optimization. Standard Adam optimizer with a cosine schedule to lower the LR for 1000 steps should be enough."
      ],
      "metadata": {
        "id": "3IZSV0C1zas0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "epochs = int(1e3)  # 1000 'epochs' seems a lot, but with single sample in the dataset, 1 epoch = 1 update step\n",
        "num_steps = len(dataloader) * epochs\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create small model with ~ 400,000 params (non embedding)\n",
        "# NOTE: Such a small model is OK since we are training on a single sample\n",
        "#       We would need to increase model size to train on the entire dataset !\n",
        "config = GPTConfig(dim=channels, n_embd=128, n_head=4, n_layer=2, dropout=0.0)\n",
        "model = GPT(config)\n",
        "model.to(device)\n",
        "\n",
        "# Since we will train on a single sample, we can use very high LR to converge faster with certain tricks\n",
        "optimizer = Adam(model.parameters(), lr=1e-2, betas=[0.9, 0.999])\n",
        "\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=100,\n",
        "    num_training_steps=num_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "DDihyFTDzOXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------\n",
        "\n",
        "You can uncomment the following line and perhaps change the path to load a pretrained checkpoint (after training at least one model)."
      ],
      "metadata": {
        "id": "RgXvmc2cRBQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load previous checkpoint if necessary\n",
        "# model.load_state_dict(torch.load(\"/content/checkpoints/jp_vowels.pt\"))"
      ],
      "metadata": {
        "id": "Mwn5cHm0APML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Training of a DDPM model is relatively simple and can be broken down into 3 steps -\n",
        "\n",
        "1) Sample a batch of input tensors $x$, a corresponding batch of integers corresponding to diffusion timesteps $t$, and gaussian noise $\\mathcal{N}(0, 1)$ of the same shape as $x$ (lets call it $ϵ$).\n",
        "\n",
        "------\n",
        "\n",
        "2) Compute $x_t$ from $x_0$, $t$ and $ϵ$ as follows -\n",
        "\n",
        "\\begin{align}\n",
        "  res &= ϵ_θ (\\sqrt{\\bar{\\alpha_t}} x_0 + \\sqrt{1 - \\bar{\\alpha_t}} ϵ, t)\n",
        "\\end{align}\n",
        "\n",
        "where $θ$ is the parameters of the model that predicts previous noise $ϵ_θ$ given the conditioning input of diffusion timestep $t$ along with the forward diffusion process at current timestep.\n",
        "\n",
        "------\n",
        "\n",
        "3) Compute the loss as follows and backprop -\n",
        "\n",
        "\\begin{align}\n",
        "  loss &= \\| \\left( ϵ - res  \\right) \\|\n",
        "\\end{align}\n",
        "\n",
        "------\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FNYcqzs41yfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a single sample, the diffusion model must learn the following (vastly simplified) -\n",
        "\n",
        "* Map a single timestep $t \\: ; 0 \\le t < T$\n",
        "* And a single input data point of shape $\\mathbb{R}^{C \\times T}$, to which noise $ϵ$ is added according to the diffusion schedule $β$\n",
        "* To $ϵ$ itself\n",
        "\n",
        "In this scenario, we have three sources of randomness -\n",
        "* The sampled input datapoint\n",
        "* The timestep $t$ that defines the diffusion timestep of this step\n",
        "* The gaussian noise that must be predicted $ϵ$\n",
        "\n",
        "During diffusion model training over real datasets, training is memory bound due to the size of the tensors for $x$ and $ϵ$, along with the model's forward-backward pass + gradients. Therefore there is no choice but to sample $t$.\n",
        "\n",
        "Intuitively, it can be considered that for a single given $x_{sample}$ - you can sample *any* $t$, and independently sample *any* $\\epsilon$ to perform a single gradient update.\n",
        "\n",
        "While $t$ is constrained to a finite set of values, $T$ can be very large (usually $1000$) and so you would need a large number of update steps to sample all possible $t$.\n",
        "\n",
        "Furthermore, for any given $t$, you can sample nearly infinite variations of gaussian noise $\\epsilon$ - so the model would need a large number of update steps if we trained it on a single $t$ and a single $ϵ$.\n",
        "\n",
        "Fortunately, in our case $x$ is a tiny tensor of just $C=12 \\times L=26$ floating point values. Furthermore, we want to train on just one single sample, so both $x_{sample}$ and $ϵ$ are both quite small.\n",
        "\n",
        "So we can dramatically improve the efficiency of training by simply computing gaussian noise value $ϵ_t$ for all given $t \\in [1,2,⋯,T]$. This way, the batch of samples now includes **all** $t$, and $t$ number of gaussian noise samples $ϵ_t \\in \\mathbb{R}^{T \\times C \\times L }$, such that the batch size is now equal to the diffusion timesteps $T$."
      ],
      "metadata": {
        "id": "podZH6JeVHKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many `t` to sample in a single step. Note that it can be used only when training a single sample at a time.\n",
        "batch_repeat = noise_scheduler.num_train_timesteps\n",
        "\n",
        "checkpoint_name = \"jp_vowels.pt\"\n",
        "\n",
        "############################################################################\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.train()\n",
        "\n",
        "# Checkpoint saving\n",
        "checkpoint_dir = '/content/checkpoints/'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "  os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "global_step = 0\n",
        "for epoch in tqdm(range(epochs), total=num_steps, desc='Epoch'):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Single sample optimization : Repeat the sample such that [1, C, T] -> [arange(0, DIFFUSION_TIMESTEPS), C, T]\n",
        "      # In doing so, we can parallely apply 1 random Diffusion `t` and `noise` to the same sample in a single step\n",
        "      # Significantly improves convergence speed by providing multiple `t` and `noise` to calculate the loss\n",
        "      if batch_repeat > 0 and batch.size(0) == 1:\n",
        "        batch = batch.expand(batch_repeat, -1, -1)\n",
        "\n",
        "      batch_size = batch.size(0)\n",
        "      batch = batch.to(device)\n",
        "\n",
        "      # Normal algorithm - sample t from discrete uniform distribution\n",
        "      # t = torch.randint(0, noise_scheduler.num_train_timesteps, (batch_size,), dtype=torch.long, device=device)\n",
        "\n",
        "      # Single sample optimization : Compute all values of T in a single update step\n",
        "      # Explicitly compute all possible T in one shot\n",
        "      t = torch.arange(0, noise_scheduler.num_train_timesteps, dtype=torch.long, device=device)\n",
        "\n",
        "      # Noise sampling per `t` - note that each `t_i` corresponds to a different noise value of shape `[1, C, T]` !\n",
        "      noise = torch.randn(batch.shape, device=device)\n",
        "      noisy_sequence = noise_scheduler.add_noise(batch, noise, t)\n",
        "\n",
        "      noise_pred = model(noisy_sequence, t)\n",
        "      loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "      if (global_step + 1) % 100 == 0:\n",
        "        print(f\"Step : {global_step + 1} - LR: {optimizer.param_groups[0]['lr']:0.6f} - Loss: {loss.item()}\")\n",
        "        torch.save(model.state_dict(), os.path.join(checkpoint_dir, checkpoint_name))\n",
        "\n",
        "      loss_val = loss.item()\n",
        "      if loss_val > 0.1:\n",
        "        print(\"Step :\", global_step + 1, \"Loss:\", loss_val)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      lr_scheduler.step()\n",
        "\n",
        "      global_step += 1"
      ],
      "metadata": {
        "id": "GzL5PaZT0t0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling to generate a single sequence\n",
        "\n",
        "Now that the model has hopefully converged, we can sample from it using the custom pipeline.\n",
        "\n",
        "Something to note : We trained the model for an exceedingly small amount of steps, and so loss is non-zero. We do expect to potentially see samples generated that do not exactly match the input sequence."
      ],
      "metadata": {
        "id": "XKkH9myu5XKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_batchsize = 4\n",
        "\n",
        "# Uncomment below to have determinstic sampling\n",
        "g = None\n",
        "# g = torch.Generator(device=device)\n",
        "# g.manual_seed(0)\n",
        "\n",
        "# Can try changing `num_inference_steps` here to check if we can diffuse a sample with fewer than DIFFUSION_TIMESTEPS steps\n",
        "pipeline = SequenceDDPMPipeline(model, noise_scheduler)\n",
        "pipeline = pipeline.to(device)\n",
        "\n",
        "pipe_output = pipeline(sample_batchsize, num_inference_steps=DIFFUSION_TIMESTEPS, generator=g, return_all=True)\n",
        "samples = pipe_output.sequences\n",
        "\n",
        "for idx, smp in enumerate(samples):\n",
        "  plt.plot(smp.reshape(channels, timesteps)[0].cpu(), label=f'Sample {idx + 1}')\n",
        "  plt.plot(sequence[0].reshape(channels, timesteps)[0].cpu(), label=f'OG Sequence {idx + 1}')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "A8vhq71i0t2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the generation history\n",
        "\n",
        "We can also observe the entire reverse diffusion process by plotting the history of a single channel over all the diffusion timesteps.\n",
        "\n",
        "**Note**: For large number of diffusion timesteps, this step could take a long time."
      ],
      "metadata": {
        "id": "nV__IM9W59Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist_sample_id = 1\n",
        "channel_id = 0\n",
        "\n",
        "################################################################################\n",
        "history = pipe_output.history\n",
        "\n",
        "assert hist_sample_id < len(history), \"hist_sample_id cannot be greater than number of generated samples !\"\n",
        "assert channel_id < channels, f\"channel_id {channel_id} cannot be greater than number of channels {channels}\"\n",
        "\n",
        "# create the figure and axes objects\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "def animate_sampling_history(i):\n",
        "  h = history[i]\n",
        "\n",
        "  ax.clear()\n",
        "  ax.set_title(f\"Sample Generated at Timestep {DIFFUSION_TIMESTEPS - i - 1}\")\n",
        "  ax.plot(h[hist_sample_id, channel_id].cpu(), label=f'Generated Sample')\n",
        "  ax.plot(sequence[0, channel_id].cpu(), label='Original Sequence')\n",
        "  ax.legend(loc=\"upper right\")\n",
        "\n",
        "# run the animation\n",
        "anim = FuncAnimation(fig, animate_sampling_history, frames=list(range(DIFFUSION_TIMESTEPS)), interval=200, repeat=False)\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "HTML(anim.to_jshtml())"
      ],
      "metadata": {
        "id": "i6eZW7lt0t4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "With this, we can now begin more advanced applications of diffusion models, such as conditional diffusion models, classifier free guidance training, continious time schedules, stochastic sampling and so on.\n",
        "\n",
        "Studying diffusion on a single sample allows us to understand the challenges of each technique, and how to successfully apply them."
      ],
      "metadata": {
        "id": "S2I-onOebS0T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yK_FfESrBYJj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}